{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fa76310",
   "metadata": {},
   "source": [
    "## 核心概念\n",
    "\n",
    "这份教程会介绍文档、语料库、向量、模型的基础概念以及与之有关的术语，用于理解和使用gensim。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2abe99e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe624a6",
   "metadata": {},
   "source": [
    "gensim核心的概念有：\n",
    "1. 文档（Document）：一些文本\n",
    "2. 语料库（Corpus）：文档的集合\n",
    "3. 向量（Vector）：文档的数学表征\n",
    "4. 模型（Model）：用于把向量从一种表征方法转移为另一种方法的算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67609f94",
   "metadata": {},
   "source": [
    "### 文档\n",
    "在gensim中，文档是一种文本序列类型的对象。文档可以是140个字符的推特，也可以是段落、新闻或者是一本书。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa1fa398",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"Human machine interface for lab abc computer applications\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac111e12",
   "metadata": {},
   "source": [
    "### 语料库\n",
    "语料库是文档对象的集合，语料库在gensim中扮演了两种角色：\n",
    "1. 作为训练模型的输入，在训练期间，模型使用训练语料库去查找常见的主题和话题，初始化模型参数；\n",
    "2. 组织文档，在训练之后，模型可以用于从新文档中提取信息。\n",
    "\n",
    "以下是一个实例语料库，包含了9个文档，每个文档仅包含一个句子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "691e41f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba98fa5",
   "metadata": {},
   "source": [
    "在收集语料库之后，还需要进行一些预处理，为了方便起见，在下面的例子中仅去除常见的英文单词（the, an等）以及在语料库中仅出现一次的单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6c846bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'],\n",
      " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'system'],\n",
      " ['system', 'human', 'system', 'eps'],\n",
      " ['user', 'response', 'time'],\n",
      " ['trees'],\n",
      " ['graph', 'trees'],\n",
      " ['graph', 'minors', 'trees'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "# Create a set of frequent words\n",
    "stoplist = set('for a of the and to in'.split(' '))\n",
    "# Lowercase each document, split it by white space and filter out stopwords\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "         for document in text_corpus]\n",
    "\n",
    "# Count word frequencies\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# Only keep words that appear more than once\n",
    "processed_corpus = [[token for token in text if frequency[token] > 1] \n",
    "                     for text in texts]\n",
    "pprint.pprint(processed_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eccfe8",
   "metadata": {},
   "source": [
    "在继续处理之前，需要将语料库中每个单词绑定一个唯一的ID号，通过使用*gensim.corpora.Dictionary*类可以实现这个操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd052051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77964dc",
   "metadata": {},
   "source": [
    "由于我们的语料库比较小，只有12个单独的token，对于大型语料库，字典包含成千上万个token是很常见的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb18ed3",
   "metadata": {},
   "source": [
    "### 向量\n",
    "为了对文档进行数学上的操作，需要对语料库进行另一种方式的表征。一个方法是使用特征向量表征每个文档。比如说，可以将一个特征看作是“问题-答案”对：\n",
    "1. 单词在文档中出现了多少次：0次\n",
    "2. 文档包含了多少个段落：2个\n",
    "3. 文档使用了多少个字体：5个\n",
    "\n",
    "问题通常由整数id表示（1，2，3等），因此文档的表征变成了一系列数字对：(1, 0.0), (2, 2.0), (3, 5.0)（对应三个问题的答案）。这就是通常所说的稠密向量，因为向量包含了所有问题的明确的答案。\n",
    "\n",
    "如果事先知道所有问题，就可以隐含问题，简单地将文档表征为（0, 2, 5)。在gensim中，只允许浮点类型的数字。在实际中，向量经常包含大量的数字0，为了解决内存，gensim删除了所有值为0的向量元素。上面的例子因此变成了(2, 2.0), (3, 5.0).这就是通常所说的稀疏向量或者词袋向量。\n",
    "\n",
    "如果问题一样，可以比较两个向量。比如说，有两个向量(0.0, 2.0, 5.0)和(0.1, 1.9, 4.9)。两个向量十分相似。\n",
    "\n",
    "另一种表征文档的方法是使用词袋模型，每个文档可以表示为每个单词的出现频率。例如，词典\\['coffee', 'milk', 'sugar', 'spoon'\\],某个文档包含字符串\"coffee milk coffee\",可以表示为向量\\[2, 1, 0, 0\\]。词袋模型完全忽略了文档token中的顺序，这也是名字“词袋”的来源。\n",
    "\n",
    "查看token对应的id："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64bfcd9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'computer': 0,\n",
      " 'eps': 8,\n",
      " 'graph': 10,\n",
      " 'human': 1,\n",
      " 'interface': 2,\n",
      " 'minors': 11,\n",
      " 'response': 3,\n",
      " 'survey': 4,\n",
      " 'system': 5,\n",
      " 'time': 6,\n",
      " 'trees': 9,\n",
      " 'user': 7}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f48dae7",
   "metadata": {},
   "source": [
    "例如，将一字符串“human computer interaction”向量化，可以使用doc2bow制造文档的词袋模型，该方法将返回词语的稀疏表征："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f51d0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1)]\n"
     ]
    }
   ],
   "source": [
    "new_doc = \"Human computer interaction\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "print(new_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68a37a3",
   "metadata": {},
   "source": [
    "每个元组的第一个元素表示了token的id（computer的id是0，human的id是1），第二个元素代表了token的出现次数。\n",
    "\n",
    "注意到interaion并没有出现在语料库中，所以向量化的结果中也不包括该单词。\n",
    "\n",
    "可以将整个原始的语料库转换为向量列表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66fa1f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1)],\n",
      " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
      " [(2, 1), (5, 1), (7, 1), (8, 1)],\n",
      " [(1, 1), (5, 2), (8, 1)],\n",
      " [(3, 1), (6, 1), (7, 1)],\n",
      " [(9, 1)],\n",
      " [(9, 1), (10, 1)],\n",
      " [(9, 1), (10, 1), (11, 1)],\n",
      " [(4, 1), (10, 1), (11, 1)]]\n"
     ]
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "pprint.pprint(bow_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86df769b",
   "metadata": {},
   "source": [
    "### 模型\n",
    "\n",
    "语料库向量化之后，可以开始使用模型。可以将模型视为在两个向量空间进行转换的算法。\n",
    "\n",
    "一个简单的模型例子是TF-IDF，将词袋表征向量空间转换为每个单词的次数频率（由相对稀有度计算而来）。\n",
    "\n",
    "以下将会初始化TF-IDF模型，在语料库上训练并且转换字符串“system minors”："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bf74a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 0.5898341626740045), (11, 0.8075244024440723)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "\n",
    "# use bow_corpus to train tfidf model\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "words = \"system minors\".lower().split()\n",
    "# 向量化\n",
    "vecs = dictionary.doc2bow(words)\n",
    "print(tfidf[vecs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65c7ec7",
   "metadata": {},
   "source": [
    "元组的第一个元素：id 5 代表system, id 11 代表minors；元组的第二个元素代表出现频率以及相对稀有度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234e039b",
   "metadata": {},
   "source": [
    "一旦创建好模型之后，可以将整个语料库通过tfidf进行索引，为相似度查询做准备："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a9eafc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.0), (1, 0.32448703), (2, 0.41707572), (3, 0.7184812), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import similarities\n",
    "\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[bow_corpus], num_features=12)\n",
    "query_document = \"system engineering\".split()\n",
    "# 向量化\n",
    "query_bow = dictionary.doc2bow(query_document)\n",
    "# 逐个查询每个文档的相似度\n",
    "sims = index[tfidf[query_bow]]\n",
    "print(list(enumerate(sims)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7148646",
   "metadata": {},
   "source": [
    "文档3拥有72%的相似度，文档2拥有42%的相似度，etc。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2818d46c",
   "metadata": {},
   "source": [
    "## 语料库和向量空间\n",
    "\n",
    "### 语料库格式\n",
    "\n",
    "gensim提供了很多保存语料库的方法，比如，将语料库保存为Market Matrix 格式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a9aca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "_corpus = [[(1, 0.5)], []]\n",
    "corpora.MmCorpus.serialize('~/corpus.mm', _corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bffa867",
   "metadata": {},
   "source": [
    "除了Market Matrix格式外，还有svmlight, lda-c, low等格式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba77070",
   "metadata": {},
   "source": [
    "## Topics and Transformations\n",
    "\n",
    "### 创建transformation\n",
    "\n",
    "首先初始化tfidf模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbedb3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98de8911",
   "metadata": {},
   "source": [
    "### Transformation 向量\n",
    "\n",
    "创建完毕后，tfidf将被视作只读对象，可以将任何向量从以前的表征方法（比如词袋）转换为tfidf表示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46c56ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5773502691896257), (1, 0.5773502691896257), (2, 0.5773502691896257)]\n",
      "[(0, 0.44424552527467476), (3, 0.44424552527467476), (4, 0.44424552527467476), (5, 0.3244870206138555), (6, 0.44424552527467476), (7, 0.3244870206138555)]\n",
      "[(2, 0.5710059809418182), (5, 0.4170757362022777), (7, 0.4170757362022777), (8, 0.5710059809418182)]\n",
      "[(1, 0.49182558987264147), (5, 0.7184811607083769), (8, 0.49182558987264147)]\n",
      "[(3, 0.6282580468670046), (6, 0.6282580468670046), (7, 0.45889394536615247)]\n",
      "[(9, 1.0)]\n",
      "[(9, 0.7071067811865475), (10, 0.7071067811865475)]\n",
      "[(9, 0.5080429008916749), (10, 0.5080429008916749), (11, 0.695546419520037)]\n",
      "[(4, 0.6282580468670046), (10, 0.45889394536615247), (11, 0.6282580468670046)]\n"
     ]
    }
   ],
   "source": [
    "corpus_tfidf = tfidf[corpus]\n",
    "for doc in corpus_tfidf:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474c2d62",
   "metadata": {},
   "source": [
    "transformation还可以被序列化，一个接着一个形成一个链,将tfidf语料通过Latent Semantic Indexing模型转换为2D空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca4b1886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.703*\"trees\" + 0.538*\"graph\" + 0.402*\"minors\" + 0.187*\"survey\" + 0.061*\"system\" + 0.060*\"time\" + 0.060*\"response\" + 0.058*\"user\" + 0.049*\"computer\" + 0.035*\"interface\"'),\n",
       " (1,\n",
       "  '0.460*\"system\" + 0.373*\"user\" + 0.332*\"eps\" + 0.328*\"interface\" + 0.320*\"response\" + 0.320*\"time\" + 0.293*\"computer\" + 0.280*\"human\" + 0.171*\"survey\" + -0.161*\"trees\"')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_model = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2)\n",
    "corpus_lsi = lsi_model[corpus_tfidf]\n",
    "\n",
    "lsi_model.print_topics(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdfbaad",
   "metadata": {},
   "source": [
    "从结果来看，\"trees\",\"graph\",\"minors\"是有关的单词。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3ca3fb",
   "metadata": {},
   "source": [
    "模型可以通过save() load()函数来实现持久化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3886b1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "with tempfile.NamedTemporaryFile(prefix='model-', suffix='.lsi', delete=False) as temp:\n",
    "    lsi_model.save(temp.name)\n",
    "    \n",
    "loaded_lsi_model = models.LsiModel.load(temp.name)\n",
    "\n",
    "os.unlink(temp.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60596e6",
   "metadata": {},
   "source": [
    "### 可用的transformation\n",
    "\n",
    "1. tfidf：\n",
    "    ```model = models.TfidfModel(corpus, normalize=True)```\n",
    "2. lsi(latent semantic indexing):\n",
    "    ```model = models.LsiModel(tfidf_corpus, id2word=dictionary, num_topics=300)```\n",
    "3. RP(random projections):\n",
    "    ```model = models.RpModel(tfidf_corpus, num_topics=300)```\n",
    "4. LDA(Latent Dirichlet Allocation):\n",
    "    ```model = models.LdaModel(corpus, id2word=dictionary, num_topics=300)```\n",
    "5. HDP(Hierachical Dirichilet Process):\n",
    "    ```model = models.HdpModel(corpus, id2word=dictionary)```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80222e05",
   "metadata": {},
   "source": [
    "## Word2Vec 模型\n",
    "\n",
    "词袋模型统计文档中单词出现的次数，并将各个单词的次数按照任意顺序作为向量元素存储在向量中，词袋模型效率高，但是丢失了单词出现顺序的信息以及不能反映语义上的区别。\n",
    "\n",
    "Word2Vec模型在低维向量空间中表征单词，并且在空间中接近的单词也具有相似的含义。\n",
    "\n",
    "使用google新闻数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f159fa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "391038d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word #0/3000000 is </s>\n",
      "word #1/3000000 is in\n",
      "word #2/3000000 is for\n",
      "word #3/3000000 is that\n",
      "word #4/3000000 is is\n",
      "word #5/3000000 is on\n",
      "word #6/3000000 is ##\n",
      "word #7/3000000 is The\n",
      "word #8/3000000 is with\n",
      "word #9/3000000 is said\n"
     ]
    }
   ],
   "source": [
    "for index, word in enumerate(wv.index_to_key):\n",
    "    if index == 10:\n",
    "        break\n",
    "    print(f\"word #{index}/{len(wv.index_to_key)} is {word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500705fa",
   "metadata": {},
   "source": [
    "word2vec的缺点是，无法infer在语料库中没有出现的单词。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5056c73",
   "metadata": {},
   "source": [
    "更进一步，word2vec支持多种单词相似度的任务："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8404f083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'car'\t'minivan'\t0.69\n",
      "'car'\t'bicycle'\t0.54\n",
      "'car'\t'airplane'\t0.42\n",
      "'car'\t'cereal'\t0.14\n",
      "'car'\t'communism'\t0.06\n"
     ]
    }
   ],
   "source": [
    "pairs = [\n",
    "    ('car', 'minivan'),\n",
    "    ('car', 'bicycle'),\n",
    "    ('car', 'airplane'),\n",
    "    ('car', 'cereal'),\n",
    "    ('car', 'communism')\n",
    "]\n",
    "\n",
    "for w1, w2, in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe01bfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('SUV', 0.8532191514968872), ('vehicle', 0.8175783753395081), ('pickup_truck', 0.7763689160346985), ('Jeep', 0.7567334175109863), ('Ford_Explorer', 0.7565719485282898)]\n"
     ]
    }
   ],
   "source": [
    "# 输出相似度最高的n个单词\n",
    "print(wv.most_similar(positive=['car', 'minivan'], topn=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
