{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99db6355",
   "metadata": {},
   "source": [
    "## 自然语言与编程语言\n",
    "\n",
    "自然语言相比人工语言（编程语言类），具有如下特点\n",
    "- 词汇量丰富：自然语言相比编程语言的关键字，词汇量更加丰富，以汉语为例，常用此表就收录了56008个词条，并且还可以随时创造新词\n",
    "- 非结构化：自然语言是非结构化的，而编程语言是结构化的，这也是处理自然语言难点的因素之一\n",
    "- 歧义性：自然语言含有大量歧义，歧义根据语境的不同而表现为特定的义项\n",
    "- 容错性：自然语言即使有一定的错误，也不影响作者想要表达的意思\n",
    "- 易变性：自然语言是不断发展变化的，而编程语言的变化则要缓慢温和得多\n",
    "- 简略性：自然语言往往简洁、干练，经常省略大量背景知识和常识"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb815a0",
   "metadata": {},
   "source": [
    "## 自然语言处理的层次\n",
    "自然语言处理系统的输入有：语音、图像与文本三个类别，语音和图像最终会转换为文本进行处理。\n",
    "\n",
    "分词、词性标注和命名实体识别统称为词法分析。\n",
    "- 中文分词：将文本分割为有意义的词语\n",
    "- 词性标注：确定每个词语的类别和浅层的歧义消除\n",
    "- 命名实体识别：识别出特定领域的专有名词\n",
    "\n",
    "词法分析之后，文本呈现出一定的机构化，每个单词还附有了自己的词性以及其他的标签，根据这些单词及标签可以抽取出一定的信息，这被称为信息抽取。\n",
    "\n",
    "判断文本的某些属性，比如判断邮件是否是垃圾邮件等，对文本进行分类的任务被称为文本分类；将具有一定相似性的文本归档，而不关心具体的类别，被称为文本聚类。\n",
    "\n",
    "词法分析只能得到零散的词汇信息，计算机不知道词语之间的联系，通过句法分析，可以得到文本的语法信息。\n",
    "\n",
    "语义分析包括了词义消歧、语义角色标注、语义依存分析，深入分析句子乃至篇章。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59503bf",
   "metadata": {},
   "source": [
    "## 语料库\n",
    "\n",
    "语料库就是自然语言处理领域中的数据集\n",
    "\n",
    "中文分词语料库指的是由人工正确切分后的句子集合，最著名的是1998年《人民日报》语料库。\n",
    "\n",
    "词性标注语料库指的是切分并为每一个词语指定一个词性的预料。\n",
    "\n",
    "命名实体识别语料库人工标注了文本内部制作者关心的实体名词以及实体类别，比如人名、机构名和地名等。\n",
    "\n",
    "句法分析语料库中每个句子都经过了分词、词性标注和句法标注，经过可视化之后可以清晰的看到单词之间的关系\n",
    "\n",
    "文本分类语料库指的是标注了所属分类的文章构成的语料库，比如汽车、财经、体育等类别。\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a747235",
   "metadata": {},
   "source": [
    "## 词典分词\n",
    "\n",
    "中文分词是将一段文本拆分为一系列单词的过程，这些单词顺序拼接后等于源文本。中文分词算法大致分为基于词典分规则与基于机器学习这两类。\n",
    "\n",
    "词典分词是最简单、最常见的分词算法，仅需一部词典和一套查词典的规则即可。\n",
    "\n",
    "## 切分算法\n",
    "\n",
    "常用的查词典规则有正向最长匹配、逆向最长匹配和双向最长匹配，都基于完全切分过程。\n",
    "\n",
    "完全切分可以找出一段文本中所有的单词。\n",
    "\n",
    "双向最长匹配同时执行正向和逆向最长匹配，若两者词数不同，则返回次数更少的那一个，否则返回单字最少的那个。\n",
    "\n",
    "匹配算法的瓶颈在于如何判断词典中是否有字符串，因此通常使用字典树存储字典，这是一种字符串上的树形数据结构。字符串就是一条路径，要查询一个单词，只需顺着路径从根节点往下走，如果能走到特殊标记（词语结尾），则说明该字符串存在集合中。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7384c46",
   "metadata": {},
   "source": [
    "## 语言模型\n",
    "\n",
    "语言模型（Language Model, LM）指的是对语言现象的数学抽象，给定一个句子w，语言模型就是计算句子的出现概率p的模型。\n",
    "\n",
    "语言模型只会估计出现在语料库中的句子，结果只符合语料库这个小型的样本空间。句子数量无穷无尽，语言模型无法全数枚举，这意味着现实中的大多数句子的概率都被当作0，这种现象被称为数据稀疏。\n",
    "\n",
    "句子无穷，但是单词却是有限的。因此可以计算单词组合的联合概率，以此推算出句子的概率。这种方法的缺点一是计算代价大，而是长句子的概率会很小，同样不能完全解决数据稀疏的问题。\n",
    "\n",
    "为了解决这两个问题，需要使用马尔可夫假设来简化语言模型：给定时间线上的一串事件顺序发生，假设每个事件的发生概率只取决于前一个事件，那么这串事件构成的因果链被称为马尔科夫链（Markov Chain）。\n",
    "\n",
    "在语言模型中，第t个事件指的就是w_t作为第t个单词出现，也就是说，马尔可夫链假设每个单词出现的概率只取决于前一个单词：\n",
    "\n",
    "$$\n",
    "p(w_t|w_0w_1 ... w_{t-1}) = p(w_t|w_{t-1})\n",
    "$$\n",
    "\n",
    "由于每次计算只涉及两个单词的二元连续，又被称为二元语法(bigram)模型。\n",
    "\n",
    "利用类似的思路，可以得到n元语法（n-gram）的定义：每个单词的概率仅取决于该单词之前的n个单词。\n",
    "\n",
    "当n=1时，称为一元语法（unigram），n=3时，称为三元语法（trigram)，n>4时，计算代价和稀疏程度变大，实际工程几乎不使用。\n",
    "\n",
    "可以使用平滑策略缓解数据稀疏问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db00003e",
   "metadata": {},
   "source": [
    "## 隐马尔可夫模型与序列标注\n",
    "\n",
    "### 序列标注问题\n",
    "\n",
    "序列标注(tagging)指的是给定一个序列x = x1x2...xn,找出序列中每个元素对应标签y = y1y2...yn的问题，其中y所有可能的取值集合称为标注集(tagset)。求解序列标注问题的模型一般称为序列标注器(tagger)，由模型从一个标注数据集中习得。在NLP中，x通常是字符或词语，而y则是词性等标签。\n",
    "\n",
    "在中文分词问题中，每个字符x_i在分词时都有切或不切两种情况，因此可以将分词问题转换为序列标注问题。\n",
    "\n",
    "事实上，分词标注集并非只有一种，为了捕捉汉字分别作为词语首尾、词中以及单字成词时不同的成词概率，人们提出了{B, E, M, S}这种最流行的标注集。\n",
    "\n",
    "词性标注任务是一个天然的序列标注问题：x是单词序列，y是相应的词性序列（名词、动词等）。最著名的词性标注集有863标注集和北大标注集。\n",
    "\n",
    "所谓命名实体(Named Entity, NE)，指的是现实存在的实体，比如人名、地名和机构名。命名实体的数量是无穷的，所以NE也是OOV(Out of Vocabulary)的主要组成部分。NER除了用到分词标注，还需要确定实体的类型，这个额外的要求依然是个标注问题。\n",
    "\n",
    "### 隐马尔可夫模型\n",
    "\n",
    "隐马尔可夫（Hidden Markov Model, HMM)是描述两个时序序列联合分布p(x,y)的概率模型：x序列外界可见，称为观测序列（Observation Sequence），y序列外界不可见，称为状态序列（State Sequence）。比如观测x为单词，状态y为词性，我们需要根据单词序列去猜测它们的词性。隐马尔可夫之所以称为隐，是因为从外界来看，其状态序列隐藏不可见，是待求的因变量。\n",
    "\n",
    "马尔科夫假设：每个事件的概率只取决于前一个事件，将满足该假设的事件连续在一起就形成了马尔科夫链。隐马尔可夫中，假设作用于状态序列，连续多个状态就构成了隐马尔可夫链y。隐马尔可夫模型还有第二个假设，任意时刻的观测x_i只依赖于该时刻的状态y_i，与其他时刻的状态或观测独立无关。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4568111",
   "metadata": {},
   "source": [
    "系统启动时进入的第一个状态称为初始状态；$y_t$确定后，会转移到状态$y_{t+1}$，根据马尔可夫假设，后者的状态只取决于前者。假设有N中状态，那么从状态$y_t$转移到状态$y_{t+1}$的概率就构成了一个NxN的方阵，称为状态转移矩阵。状态转移的概率具有实际意义，比如标签B（词的开头）的后面不可能是S（单字成词），或者在词性标注中，形容词后面是名词等。\n",
    "\n",
    "有了状态$y_t$后，如何确定$x_t$的概率分布呢？根据隐马尔可夫假设②，当前观测状态只取决于当前状态$y_t$**（NLP中认为x取决于y，比如写文章时构思好一个满足语法的词性序列，在填充为具体的词语）**。假设x一共有M种可能的取值，即M种字符，则x的概率分布参数向量维度为M，y一种有N种（比如说N种词性），则这些参数向量构成了NxM的矩阵，称此矩阵为发射概率矩阵B。\n",
    "\n",
    "发射概率矩阵：将$y_t$想象为不同的彩弹枪，$x_t$想象为不同颜色的子弹，则根据$y_t$确定$x_t$的过程就像拔枪发射彩弹一样。不同的彩弹枪内每种彩弹比例不同，导致某些彩弹枪更可能发射红色彩弹。\n",
    "\n",
    "在中文分词实际中，“忐忑”更容易一起出现，被分为一个词\n",
    "\n",
    "初始状态概率向量、状态转移概率矩阵和发射概率矩阵被称为隐马尔可夫模型的三元组\n",
    "$$\n",
    "\\lambda=(\\pi,A,B)\n",
    "$$\n",
    "，**只要三元组确定了，隐马尔可夫模型就确定了**。\n",
    "\n",
    "隐马尔可夫模型的作用并不仅限于预测标注序列，它一共解决如下三个问题：\n",
    "1. 样本生成问题，给定隐马尔可夫模型，生成满足模型约束的样本（文本生成）\n",
    "2. 模型训练问题，给定训练集，估计模型参数$\\lambda$\n",
    "3. 序列预测问题，已知模型参数，给定观测序列x，求最可能的状态序列y\n",
    "\n",
    "如果隐马尔可夫模型种的每个状态仅依赖于钱一个状态，则称为一阶，如果依赖前两个状态，则称为二阶。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dba4c63",
   "metadata": {},
   "source": [
    "## 感知机分类与序列标注\n",
    "\n",
    "分类指的是预测样本所属类别的一类问题，目标是给定输入样本x，将其分配给K种类中的一种，如果K=2，则称为二元类(Binary classification)，否则称为多分类(multiclass classification)。\n",
    "\n",
    "当然，二分类也可以解决任意类别数的多分类问题，具体来说，有one-vs-one和one-vs-rest(也称为one-vs-all)两种方案：\n",
    "- one-vs-one: 进行多轮二分类，每次区别两种类别，一共进行$C_k^2$次分类，理想情况下有且仅有一种类别$L_i$每次都胜出，于是结果为$L_i$\n",
    "- one-vs-rest:依然是多轮二分类，每次区分两种类别，一共进行K次二分类，理想情况下模型给予$L_i的分数是所有K次分类中的最高值，于是预测结果为$L_i$.\n",
    "\n",
    "可见二分类是分类问题的基础，任何分类模型只要能解决二分类，就能用于多分类。\n",
    "\n",
    "### 线性分类模型与感知机算法\n",
    "\n",
    "线性模型(linear model)是传统机器学习方法中最简单最常用的分类模型，用一条线性直线或高维平面将数据一分为二，线性模型由特征函数$\\theta$，以及相应的权重向量w组成。\n",
    "\n",
    "二元语法和隐马尔可夫模型的学习算法，它们是计数式的训练算法：统计训练集上各事件的发生次数，然后利用极大似然估计归一化频率后得到相关概率，这些概率就是学习到的模型参数。\n",
    "\n",
    "感知机算法（perceptron algorithm）则是一种迭代式的算法：在训练集上运行多个迭代，每次读入一个样本，执行预测、将预测结果与正确答案进行对比，计算误差，根据误差更新模型参数：\n",
    "1. 读入训练样本xi,yi，执行预测y_hat\n",
    "2. 如果y_hat != y，则更新参数w\n",
    "\n",
    "在训练集的每个样本上执行步骤（1）和（2）称作一次在线学习，把训练集完整地学习一遍称作一次迭代(epoch)。\n",
    "\n",
    "从数值优化地角度来讲，迭代式机器学习算法都在优化（减小）一个损失函数（loss function），损失函数J(w)用来衡量模型在训练集上的错误程度，自变量是模型参数w,因变量是要给标量，表示模型在训练集上的损失大小，给定某一个样本，其特征向量$x^{i}$只是常数，对J(w)求导，得到一个梯度向量，它的反方向一定是当前位置损失函数减小速度最快的方向。如果算法每次迭代随机选取部分样本，计算损失函数的梯度，让参数反向移动，则称作随机梯度下降（Stochastic Gradient Descent, SGD)；另外一个情景，希望让目标函数最大化，此时参数更新方向就是梯度方向，亦即让参数加上梯度，目标函数就会增大，称作随机梯度上升（Stochasitc Gradient Ascend）。\n",
    "\n",
    "在机器学习中，提高系统准确率无非三种方法：\n",
    "1. 特征工程，即修改特征模板\n",
    "2. 切换训练算法\n",
    "3. 收集标注更多数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbba1429",
   "metadata": {},
   "source": [
    "自然语言处理问题大致可以分为两类，一种是分类问题，另一种就是结构化预测问题，序列标注只是结构化预测的一个特征。分类问题的预测结果是一个决策边界，回归问题的预测结果是一个实数标量，而结构化预测结果则是一个完整的结构，可见结构化预测难度更高。自然语言处理中有许多任务是结构化预测，比如序列标注预测结构是一整个序列，句法分析预测结构是一颗句法树，机器翻译预测机构是一段完整的译文。\n",
    "\n",
    "结构化预测的过程就是给定一个模型及打分函数score，利用打分函数给一些备选结构打分，选择分数最高的结构作为预测输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8fd996",
   "metadata": {},
   "source": [
    "### 线性模型的结构化感知机算法\n",
    "要让线性模型支持结构化预测，必须先设计打分函数，打分函数的输入有两个缺一不可的参数：特征x和结构y，但之前介绍的线性模型的“打分函数”只接受一个自变量x：\n",
    "\n",
    "$$\n",
    "f(x) = wx\n",
    "$$\n",
    "\n",
    "因此，定义新的特征函数$\\theta(x,y)$，把机构y也作为一种特征，输出新的结构化特征向量，新特征向量与权重向量作点积后，就得到一个标量，将其作为分数：\n",
    "\n",
    "$$\n",
    "score(x,y) = w·\\theta(x,y)\n",
    "$$\n",
    "\n",
    "打分函数有了，取分值最大的结构作为预测结果。\n",
    "\n",
    "使用结构化感知机用来实现序列标注，类比于隐马尔科夫模型，将隐马尔可夫模型的初始状态概率向量和概率矩阵层叠起来，恰好是一个(N+1)\\*N的矩阵，该矩阵与转移特征所起的作用是一致的，两者都捕捉了从一个状态转移到另一个状态的规律。同理，隐马尔科夫模型的发射矩阵与结构化感知机的状态特征道理相同，两者都在模拟“状态”与“观测”的相互联系。\n",
    "\n",
    "于是，结构化感知机的特征函数就是转移特征和状态特征的合集，整个序列的分数是各个时刻的得分之和：\n",
    "\n",
    "$$\n",
    "score(x,y)=\\sum_{t=1}^{T}w·\\theta(y_{t-1},y_t,x_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05123bda",
   "metadata": {},
   "source": [
    "## 条件随机场与序列标注\n",
    "\n",
    "生成式模型模拟数据的生成过程，两类随机变量存在因果先后关系：现有因素y，后有结果x。生成式模型忽略了特征之间的联系，导致结果与现实不符合，准确率不高。判别式模型诸如感知机、条件随机场、支持向量机等都考虑了特征之间的依赖关系，适用于各种各样丰富的、有关联的特征。\n",
    "\n",
    "无论是生成式还是判别式，模型都在建模多维随机变量分布，这些随机变量可能彼此独立，也可能相互依赖，构成错综复杂的关系，为了分析多维随机变量分布，人们使用概率图模型这个工具。\n",
    "\n",
    "概率图模型（Probabilistic Graphical Model, PGM）用来表示与推断多维随机变量联合分布p(x,y)的强大工具，它利用节点V来表示随机变量，用边E联接有关联的随机变量，将多维随机变量分布表示为图G=(V,E),这样带来了一个好处，那就是整个图可以分解为子图再进行分析，子图中的随机变量更少，建模更加简单。\n",
    "\n",
    "有向图模型（Directed Graphical Model, DGM)按事件的先后因果顺序将节点连接为有向图，如果事件A导致了B，则用箭头由A指向B。有向图模型经常使用生成式模型（马尔可夫模型）来实现。\n",
    "\n",
    "相反，无向图模型则不探究每个事件的因果关系，也就是说不涉及条件概率分解，无向图模型的边没有反向，仅仅代表两个事件有关联，不表示因果关系。在图论中，最大团(maximal clique)指的是满足所有节点相互连接的最大子图，无向图模型将概率分解为所有最大团上的某种函数之积。最大团无法分解，必须同时考虑所有变量，因此分析很复杂，为此，无向图模型定义了一些**虚拟的因子节点**，每个因子节点只连接部分节点，组成更小的团，方便进行分析计算。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97246973",
   "metadata": {},
   "source": [
    "### 条件随机场\n",
    "\n",
    "条件随机场（Conditional Random Field, CRF）是一种给定输入随机变量x，求解条件概率的概率无向图模型，用于序列标注时，特征化为线性链（linear-chain）条件随机场。\n",
    "\n",
    "每个样本$x_i$可以拥有多个特征，相较于隐马尔科夫模型对样本提取的唯一特征，条件随机场可利用的特征更加丰富，因子节点可以理解为一个特征函数$f(y_{t-1},y_t,x_t)$，其中利用了x_t和y_t的特征称为状态特征，利用了$y_{t-1}$的特征称为转移特征，与感知机的定义相同。\n",
    "\n",
    "条件随机场和结构化感知机具有一定的联系：\n",
    "1. 条件随机场和结构化感知机的特征函数完全一致；\n",
    "2. 结构化感知机对某预测打分越高，条件随机场给出的概率就越大；\n",
    "3. 预测算法相同，都是维特比算法；\n",
    "4. 同属结构化学习；\n",
    "5. 最大的不同在于训练算法。\n",
    "\n",
    "首先，感知机算法属于在线学习，每次更新只使用一个训练实例，由于没有考虑整个数据集，所以在线学习难免顾此失彼，而条件随机场对数似然函数机器梯度则定义在整个数据集上，每次参数更新都是全盘考虑。即：感知机奖励正确答案，但仅惩罚错得最厉害的那一个，而条件随机场不但奖励正确答案对应的特征函数，还同时惩罚所有答案y，这样就导致所有错误结果对应的特征函数都受到惩罚。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26b7e43",
   "metadata": {},
   "source": [
    "## 常见语言任务\n",
    "### 词性标注\n",
    "\n",
    "在语言学上，词性指的是单词的语法分类，也称为词类，同一个类别的词语具有相似的语法性质，所有词性的集合称为词性标注集，不同的语料库采用了不同的词性标注集，一般都有形容词、动词、名词等。词性的用处之一就是猜测OOV的用法，或者提取信息。\n",
    "\n",
    "### 命名实体识别\n",
    "\n",
    "文本中有一些描述实体的词汇，比如人名、地名、组织机构名等，称为命名实体（named entity），命名实体是人们最关注的词汇，往往是信息抽取任务的焦点。\n",
    "\n",
    "命名实体具有如下共性：\n",
    "1. 数量无穷\n",
    "2. 构词灵活，比如中国工商银行，也可以简称为工行\n",
    "3. 类别模糊，比如地名也可以是机构名\n",
    "\n",
    "由于以上难点，命名实体识别任务往往也是一个统计为主，规则为辅的任务。\n",
    "\n",
    "### 信息抽取\n",
    "\n",
    "新词提取：\n",
    "1. 提取出大量文本中的词语，无论新旧；\n",
    "2. 用词典过滤掉已有的词语，于是得到新词。\n",
    "\n",
    "关键步骤是①，怎样提取词语。在信息论中，信息熵（entropy）指的是某条消息所含的信息量，反映了听说某个信息之后，关于该事件的不确定性的减少量。在新词提取任务中，给定字符串S作为词语备选，X作为该字符串左边可能出现的字符，则称H(x)为S的左信息熵，类似地，定义右信息熵H(y)。左右信息熵越大，说明字符串的搭配越丰富，该字符串就有很大的可能是一个词语。\n",
    "\n",
    "当然，光是考虑左右信息熵是不够的，为了更好的效果，还应该考虑词语内部片段的凝聚程度，这种凝聚程度由互信息衡量。\n",
    "\n",
    "互信息(Mutual Information)指的是两个离散型随机变量X和Y相关程度的度量。两个随机变量的关联越密切，互信息就越大。具体到新词提取任务中，随机变量X代表字符串的一个子串，Y代表剩下的那部分子串，计算它们的互信息，互信息较高，则可以提高判断字符串是词语的置信度。\n",
    "\n",
    "**关键词提取**：提取文章中重要的单词，而不限于其新鲜程度。往往采用无监督算法实现，比如词频、TF-IDF、TextRank等。\n",
    "\n",
    "通过统计文章中每种词语的词频并排序，可以初步获取部分关键词，不过文章中反复出现的就不一定是关键词，比如助词“的”，为了排除这些意义不大的词语，词频统计之前通常需要**停用词过滤**。词频统计的流程一般是分词、停用词过滤、按词频提取前n个。\n",
    "\n",
    "TF-IDF（Term Frequency-Inverse Document Frequency，词频-倒排文档频次）是信息检索中衡量一个词语重要程度的统计指标，被广泛用于搜素引擎。相较于词频，TF-IDF还综合考虑了词语的稀有程度，一个词语的重要程度不光正比于它在文档中的频次，还反比于有多少文档包含它，包含该词语的文档越多，就说明它越宽泛，越不能体现文档的特色。\n",
    "\n",
    "TextRank与PageRank相同，PageRank是一种用于网页排序的算法。网站给别的网站做外链越多，每条外链的权重就越低，如果一个网站的外链都是这种权重很低的外链，那么PageRank就会下降。将PageRank应用到关键词提取，就是TextRank了。对于某个单词，它的外链来自以它为一定半径内的所有词语，不同的词语越多，则该单词的“外链”数目就越大。TextRank不仅考虑到了频次，还考虑到了单词的权重，经常与其他词语组合的单词（外链数目很大），其权重肯定很低，比如“非常”、“特别”这类经常出现的词语。\n",
    "\n",
    "**短语提取**：在信息抽取领域，另一项重要的任务，也即固定多字词表达串的识别。短语提取经常用于搜索引擎的自动推荐，文档的简介生成等。利用互信息和左右信息熵（用于词语提取），可以轻松地将新词提取算法扩展到短语提取，只需将字符替换为单词，字符串替换为单词列表即可。\n",
    "\n",
    "**关键句提取**：由于一篇文章中几乎不可能出现相同的两个句子，所以PageRank在句子颗粒度上行不通。为了将PageRank利用到句子颗粒度上去，引入了BM25算法来衡量句子的相似度。BM25是TF-IDF的一种改进变种，TF-IDF衡量的是单个词语在文档中的重要程度，而在搜索引擎中，查询串往往是由多个词语构成的，如何衡量多个词语与文档的关联程度，就是BM25所解决的问题。有了BM25算法之后，将一个句子视作查询语句，相邻的句子视作待查询的文档，就能得到它们之间的相似度，一次相似度作为PageRank中的链接权重，于是得到一个改进算法，称为TextRank。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc3f4a5",
   "metadata": {},
   "source": [
    "## 文本聚类\n",
    "\n",
    "聚类（cluster analysis）指的是将给定对象的集合划分为不同子集的过程，目标是使每个子集内部的元素尽量相似，不同子集间的元素尽量不相似。这些子集又被称为簇。\n",
    "\n",
    "根据元素从属于集合的确定程度，又分为硬聚类和软聚类：\n",
    "- 硬聚类（hard clustering）：每个元素被确定的归入一个簇\n",
    "- 软聚类（soft clustering）：每个元素与每个簇都有一定的从属关系\n",
    "\n",
    "在硬聚类中，从属关系是离散的，非常强硬（1，0），在软聚类中，从属关系则用一个连续值来衡量，非常灵活。\n",
    "\n",
    "聚类通常用于数据的预处理，或归档相似的数据。\n",
    "\n",
    "### 文档的特征提取\n",
    "\n",
    "词袋（bag of words）是信息检索与自然语言处理中最常用的文档表示模型，它将文档想象为一个装有词语的袋子，通过袋子中每种词语的计数等统计量将文档表示为向量。常见统计量有：\n",
    "1. 布尔词频：词频非零就记为1\n",
    "2. TF-IDF\n",
    "3. 词向量\n",
    "4. 词频\n",
    "\n",
    "词频向量适合主题较多的数据集，布尔词频适合长度较短的数据集，TF-IDF适合主题较少的数据集，而词向量适合处理OOV问题严重的数据集。\n",
    "\n",
    "\n",
    "### 聚类算法\n",
    "\n",
    "k均值：以最小化每个向量到质心的欧拉距离的平方和为准则进行聚类\n",
    "重复二分聚类算法：反复对子集进行二分\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b32ff7",
   "metadata": {},
   "source": [
    "## 文本分类\n",
    "\n",
    "文本分类又称为文档分类，指的是将一个文档归类到一个或多个类别。文档分类的应用场景非常广泛，涵盖垃圾邮件过滤、垃圾评论过滤、自动标签等。\n",
    "\n",
    "对文档进行特征提取，一般采用词袋向量作为特征向量。\n",
    "\n",
    "在各种各样的分类器中，朴素贝叶斯可算是最简单最常用的一种生成式模型。朴素贝叶斯基于贝叶斯定理将联合概率转化为条件概率，然后利用特征条件独立假设简化条件概率的计算。\n",
    "\n",
    "支持向量机是一种二分类模型，其学习策略在于如何找出一个决策边界，使得边界到正负样本的最小距离都最近，适用于线性可分数据集。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
