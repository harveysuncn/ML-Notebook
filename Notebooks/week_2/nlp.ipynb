{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99db6355",
   "metadata": {},
   "source": [
    "## 自然语言与编程语言\n",
    "\n",
    "自然语言相比人工语言（编程语言类），具有如下特点\n",
    "- 词汇量丰富：自然语言相比编程语言的关键字，词汇量更加丰富，以汉语为例，常用此表就收录了56008个词条，并且还可以随时创造新词\n",
    "- 非结构化：自然语言是非结构化的，而编程语言是结构化的，这也是处理自然语言难点的因素之一\n",
    "- 歧义性：自然语言含有大量歧义，歧义根据语境的不同而表现为特定的义项\n",
    "- 容错性：自然语言即使有一定的错误，也不影响作者想要表达的意思\n",
    "- 易变性：自然语言是不断发展变化的，而编程语言的变化则要缓慢温和得多\n",
    "- 简略性：自然语言往往简洁、干练，经常省略大量背景知识和常识"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb815a0",
   "metadata": {},
   "source": [
    "## 自然语言处理的层次\n",
    "自然语言处理系统的输入有：语音、图像与文本三个类别，语音和图像最终会转换为文本进行处理。\n",
    "\n",
    "分词、词性标注和命名实体识别统称为词法分析。\n",
    "- 中文分词：将文本分割为有意义的词语\n",
    "- 词性标注：确定每个词语的类别和浅层的歧义消除\n",
    "- 命名实体识别：识别出特定领域的专有名词\n",
    "\n",
    "词法分析之后，文本呈现出一定的机构化，每个单词还附有了自己的词性以及其他的标签，根据这些单词及标签可以抽取出一定的信息，这被称为信息抽取。\n",
    "\n",
    "判断文本的某些属性，比如判断邮件是否是垃圾邮件等，对文本进行分类的任务被称为文本分类；将具有一定相似性的文本归档，而不关心具体的类别，被称为文本聚类。\n",
    "\n",
    "词法分析只能得到零散的词汇信息，计算机不知道词语之间的联系，通过句法分析，可以得到文本的语法信息。\n",
    "\n",
    "语义分析包括了词义消歧、语义角色标注、语义依存分析，深入分析句子乃至篇章。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59503bf",
   "metadata": {},
   "source": [
    "## 语料库\n",
    "\n",
    "语料库就是自然语言处理领域中的数据集\n",
    "\n",
    "中文分词语料库指的是由人工正确切分后的句子集合，最著名的是1998年《人民日报》语料库。\n",
    "\n",
    "词性标注语料库指的是切分并为每一个词语指定一个词性的预料。\n",
    "\n",
    "命名实体识别语料库人工标注了文本内部制作者关心的实体名词以及实体类别，比如人名、机构名和地名等。\n",
    "\n",
    "句法分析语料库中每个句子都经过了分词、词性标注和句法标注，经过可视化之后可以清晰的看到单词之间的关系\n",
    "\n",
    "文本分类语料库指的是标注了所属分类的文章构成的语料库，比如汽车、财经、体育等类别。\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a747235",
   "metadata": {},
   "source": [
    "## 词典分词\n",
    "\n",
    "中文分词是将一段文本拆分为一系列单词的过程，这些单词顺序拼接后等于源文本。中文分词算法大致分为基于词典分规则与基于机器学习这两类。\n",
    "\n",
    "词典分词是最简单、最常见的分词算法，仅需一部词典和一套查词典的规则即可。\n",
    "\n",
    "## 切分算法\n",
    "\n",
    "常用的查词典规则有正向最长匹配、逆向最长匹配和双向最长匹配，都基于完全切分过程。\n",
    "\n",
    "完全切分可以找出一段文本中所有的单词。\n",
    "\n",
    "双向最长匹配同时执行正向和逆向最长匹配，若两者词数不同，则返回次数更少的那一个，否则返回单字最少的那个。\n",
    "\n",
    "匹配算法的瓶颈在于如何判断词典中是否有字符串，因此通常使用字典树存储字典，这是一种字符串上的树形数据结构。字符串就是一条路径，要查询一个单词，只需顺着路径从根节点往下走，如果能走到特殊标记（词语结尾），则说明该字符串存在集合中。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7384c46",
   "metadata": {},
   "source": [
    "## 语言模型\n",
    "\n",
    "语言模型（Language Model, LM）指的是对语言现象的数学抽象，给定一个句子w，语言模型就是计算句子的出现概率p的模型。\n",
    "\n",
    "语言模型只会估计出现在语料库中的句子，结果只符合语料库这个小型的样本空间。句子数量无穷无尽，语言模型无法全数枚举，这意味着现实中的大多数句子的概率都被当作0，这种现象被称为数据稀疏。\n",
    "\n",
    "句子无穷，但是单词却是有限的。因此可以计算单词组合的联合概率，以此推算出句子的概率。这种方法的缺点一是计算代价大，而是长句子的概率会很小，同样不能完全解决数据稀疏的问题。\n",
    "\n",
    "为了解决这两个问题，需要使用马尔可夫假设来简化语言模型：给定时间线上的一串事件顺序发生，假设每个事件的发生概率只取决于前一个事件，那么这串事件构成的因果链被称为马尔科夫链（Markov Chain）。\n",
    "\n",
    "在语言模型中，第t个事件指的就是w_t作为第t个单词出现，也就是说，马尔可夫链假设每个单词出现的概率只取决于前一个单词：\n",
    "\n",
    "$$\n",
    "p(w_t|w_0w_1 ... w_{t-1}) = p(w_t|w_{t-1})\n",
    "$$\n",
    "\n",
    "由于每次计算只涉及两个单词的二元连续，又被称为二元语法(bigram)模型。\n",
    "\n",
    "利用类似的思路，可以得到n元语法（n-gram）的定义：每个单词的概率仅取决于该单词之前的n个单词。\n",
    "\n",
    "当n=1时，称为一元语法（unigram），n=3时，称为三元语法（trigram)，n>4时，计算代价和稀疏程度变大，实际工程几乎不使用。\n",
    "\n",
    "可以使用平滑策略缓解数据稀疏问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db00003e",
   "metadata": {},
   "source": [
    "## 隐马尔可夫模型与序列标注\n",
    "\n",
    "### 序列标注问题\n",
    "\n",
    "序列标注(tagging)指的是给定一个序列x = x1x2...xn,找出序列中每个元素对应标签y = y1y2...yn的问题，其中y所有可能的取值集合称为标注集(tagset)。求解序列标注问题的模型一般称为序列标注器(tagger)，由模型从一个标注数据集中习得。在NLP中，x通常是字符或词语，而y则是词性等标签。\n",
    "\n",
    "在中文分词问题中，每个字符x_i在分词时都有切或不切两种情况，因此可以将分词问题转换为序列标注问题。\n",
    "\n",
    "事实上，分词标注集并非只有一种，为了捕捉汉字分别作为词语首尾、词中以及单字成词时不同的成词概率，人们提出了{B, E, M, S}这种最流行的标注集。\n",
    "\n",
    "词性标注任务是一个天然的序列标注问题：x是单词序列，y是相应的词性序列（名词、动词等）。最著名的词性标注集有863标注集和北大标注集。\n",
    "\n",
    "所谓命名实体(Named Entity, NE)，指的是现实存在的实体，比如人名、地名和机构名。命名实体的数量是无穷的，所以NE也是OOV(Out of Vocabulary)的主要组成部分。NER除了用到分词标注，还需要确定实体的类型，这个额外的要求依然是个标注问题。\n",
    "\n",
    "### 隐马尔可夫模型\n",
    "\n",
    "隐马尔可夫（Hidden Markov Model, HMM)是描述两个时序序列联合分布p(x,y)的概率模型：x序列外界可见，称为观测序列（Observation Sequence），y序列外界不可见，称为状态序列（State Sequence）。比如观测x为单词，状态y为词性，我们需要根据单词序列去猜测它们的词性。隐马尔可夫之所以称为隐，是因为从外界来看，其状态序列隐藏不可见，是待求的因变量。\n",
    "\n",
    "马尔科夫假设：每个事件的概率只取决于前一个事件，将满足该假设的事件连续在一起就形成了马尔科夫链。隐马尔可夫中，假设作用于状态序列，连续多个状态就构成了隐马尔可夫链y。隐马尔可夫模型还有第二个假设，任意时刻的观测x_i只依赖于该时刻的状态y_i，与其他时刻的状态或观测独立无关。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4568111",
   "metadata": {},
   "source": [
    "系统启动时进入的第一个状态称为初始状态；$y_t$确定后，会转移到状态$y_{t+1}$，根据马尔可夫假设，后者的状态只取决于前者。假设有N中状态，那么从状态$y_t$转移到状态$y_{t+1}$的概率就构成了一个NxN的方阵，称为状态转移矩阵。状态转移的概率具有实际意义，比如标签B（词的开头）的后面不可能是S（单字成词），或者在词性标注中，形容词后面是名词等。\n",
    "\n",
    "有了状态$y_t$后，如何确定$x_t$的概率分布呢？根据隐马尔可夫假设②，当前观测状态只取决于当前状态$y_t$**（NLP中认为x取决于y，比如写文章时构思好一个满足语法的词性序列，在填充为具体的词语）**。假设x一共有M种可能的取值，即M种字符，则x的概率分布参数向量维度为M，y一种有N种（比如说N种词性），则这些参数向量构成了NxM的矩阵，称此矩阵为发射概率矩阵B。\n",
    "\n",
    "发射概率矩阵：将$y_t$想象为不同的彩弹枪，$x_t$想象为不同颜色的子弹，则根据$y_t$确定$x_t$的过程就像拔枪发射彩弹一样。不同的彩弹枪内每种彩弹比例不同，导致某些彩弹枪更可能发射红色彩弹。\n",
    "\n",
    "在中文分词实际中，“忐忑”更容易一起出现，被分为一个词\n",
    "\n",
    "初始状态概率向量、状态转移概率矩阵和发射概率矩阵被称为隐马尔可夫模型的三元组\n",
    "$$\n",
    "\\lambda=(\\pi,A,B)\n",
    "$$\n",
    "，**只要三元组确定了，隐马尔可夫模型就确定了**。\n",
    "\n",
    "隐马尔可夫模型的作用并不仅限于预测标注序列，它一共解决如下三个问题：\n",
    "1. 样本生成问题，给定隐马尔可夫模型，生成满足模型约束的样本（文本生成）\n",
    "2. 模型训练问题，给定训练集，估计模型参数$\\lambda$\n",
    "3. 序列预测问题，已知模型参数，给定观测序列x，求最可能的状态序列y\n",
    "\n",
    "如果隐马尔可夫模型种的每个状态仅依赖于钱一个状态，则称为一阶，如果依赖前两个状态，则称为二阶。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dba4c63",
   "metadata": {},
   "source": [
    "## 感知机分类与序列标注\n",
    "\n",
    "分类指的是预测样本所属类别的一类问题，目标是给定输入样本x，将其分配给K种类中的一种，如果K=2，则称为二元类(Binary classification)，否则称为多分类(multiclass classification)。\n",
    "\n",
    "当然，二分类也可以解决任意类别数的多分类问题，具体来说，有one-vs-one和one-vs-rest(也称为one-vs-all)两种方案：\n",
    "- one-vs-one: 进行多轮二分类，每次区别两种类别，一共进行$C_k^2$次分类，理想情况下有且仅有一种类别$L_i$每次都胜出，于是结果为$L_i$\n",
    "- one-vs-rest:依然是多轮二分类，每次区分两种类别，一共进行K次二分类，理想情况下模型给予$L_i的分数是所有K次分类中的最高值，于是预测结果为$L_i$.\n",
    "\n",
    "可见二分类是分类问题的基础，任何分类模型只要能解决二分类，就能用于多分类。\n",
    "\n",
    "### 线性分类模型与感知机算法\n",
    "\n",
    "线性模型(linear model)是传统机器学习方法中最简单最常用的分类模型，用一条线性直线或高维平面将数据一分为二，线性模型由特征函数$\\theta$，以及相应的权重向量w组成。\n",
    "\n",
    "二元语法和隐马尔可夫模型的学习算法，它们是计数式的训练算法：统计训练集上各事件的发生次数，然后利用极大似然估计归一化频率后得到相关概率，这些概率就是学习到的模型参数。\n",
    "\n",
    "感知机算法（perceptron algorithm）则是一种迭代式的算法：在训练集上运行多个迭代，每次读入一个样本，执行预测、将预测结果与正确答案进行对比，计算误差，根据误差更新模型参数：\n",
    "1. 读入训练样本xi,yi，执行预测y_hat\n",
    "2. 如果y_hat != y，则更新参数w\n",
    "\n",
    "在训练集的每个样本上执行步骤（1）和（2）称作一次在线学习，把训练集完整地学习一遍称作一次迭代(epoch)。\n",
    "\n",
    "从数值优化地角度来讲，迭代式机器学习算法都在优化（减小）一个损失函数（loss function），损失函数J(w)用来衡量模型在训练集上的错误程度，自变量是模型参数w,因变量是要给标量，表示模型在训练集上的损失大小，给定某一个样本，其特征向量$x^{i}$只是常数，对J(w)求导，得到一个梯度向量，它的反方向一定是当前位置损失函数减小速度最快的方向。如果算法每次迭代随机选取部分样本，计算损失函数的梯度，让参数反向移动，则称作随机梯度下降（Stochastic Gradient Descent, SGD)；另外一个情景，希望让目标函数最大化，此时参数更新方向就是梯度方向，亦即让参数加上梯度，目标函数就会增大，称作随机梯度上升（Stochasitc Gradient Ascend）。\n",
    "\n",
    "在机器学习中，提高系统准确率无非三种方法：\n",
    "1. 特征工程，即修改特征模板\n",
    "2. 切换训练算法\n",
    "3. 收集标注更多数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbba1429",
   "metadata": {},
   "source": [
    "自然语言处理问题大致可以分为两类，一种是分类问题，另一种就是结构化预测问题，序列标注只是结构化预测的一个特征。分类问题的预测结果是一个决策边界，回归问题的预测结果是一个实数标量，而结构化预测结果则是一个完整的结构，可见结构化预测难度更高。自然语言处理中有许多任务是结构化预测，比如序列标注预测结构是一整个序列，句法分析预测结构是一颗句法树，机器翻译预测机构是一段完整的译文。\n",
    "\n",
    "结构化预测的过程就是给定一个模型及打分函数score，利用打分函数给一些备选结构打分，选择分数最高的结构作为预测输出。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
